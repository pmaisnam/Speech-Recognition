{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train freq\n",
      "  right  1894\n",
      "   left  1900\n",
      "    off  1883\n",
      "     no  1914\n",
      "     up  1903\n",
      "     go  1917\n",
      "    yes  1906\n",
      "     on  1906\n",
      "   down  1906\n",
      "unknown  2196\n",
      "silence  341\n",
      "   stop  1919\n",
      "\n",
      "val freq\n",
      "  right  473\n",
      "   left  453\n",
      "    off  474\n",
      "     no  461\n",
      "     up  472\n",
      "     go  455\n",
      "    yes  471\n",
      "     on  461\n",
      "   down  453\n",
      "unknown  670\n",
      "silence  57\n",
      "   stop  461\n",
      "\n",
      "test files 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "from pathlib import Path\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import librosa\n",
    "import os\n",
    "import multiprocessing.pool\n",
    "from functools import partial\n",
    "from random import getrandbits\n",
    "\n",
    "train_path='../speech/train/'\n",
    "val_path ='../speech/val/'\n",
    "test_path ='../speech/'\n",
    "\n",
    "classnames=os.listdir(train_path)\n",
    "train_count_dict = {}\n",
    "for d in classnames:\n",
    "    train_count_dict[d] = len(os.listdir(os.path.join(train_path, d)))\n",
    "print('train freq')\n",
    "for k, v in train_count_dict.items():\n",
    "    print ( '%7s  %i' % (k, v))\n",
    "val_count_dict = {}\n",
    "for d in classnames:\n",
    "    val_count_dict[d] = len(os.listdir(os.path.join(val_path, d)))\n",
    "print('\\nval freq')\n",
    "for k, v in val_count_dict.items():\n",
    "    print ( '%7s  %i' % (k, v))\n",
    "print ('')\n",
    "print ('test files', len(os.listdir(test_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spect_loader(path, window_size, window_stride, window, normalize, max_len=101, \n",
    "                 augment=False, allow_speedandpitch=False, allow_pitch=False,\n",
    "                 allow_speed=False, allow_dyn=False, allow_noise=False,\n",
    "                allow_timeshift=False ):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    # n_fft = 4096\n",
    "    n_fft = int(sr * window_size)\n",
    "    win_length = n_fft\n",
    "    hop_length = int(sr * window_stride)\n",
    "\n",
    "    # STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                     win_length=win_length, window=window)\n",
    "    spect, phase = librosa.magphase(D)\n",
    "\n",
    "    # S = log(S+1)\n",
    "    spect = np.log1p(spect)\n",
    "\n",
    "    # make all spects with the same dims\n",
    "    # TODO: change that in the future\n",
    "    if spect.shape[1] < max_len:\n",
    "        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n",
    "        spect = np.hstack((spect, pad))\n",
    "    elif spect.shape[1] > max_len:\n",
    "        spect = spect[:max_len, ]\n",
    "    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n",
    "    #spect = torch.FloatTensor(spect)\n",
    "\n",
    "    # z-score normalization\n",
    "    if normalize:\n",
    "        mean = np.mean(np.ravel(spect))\n",
    "        std = np.std(np.ravel(spect))\n",
    "        if std != 0:\n",
    "            spect = spect -mean\n",
    "            spect = spect / std\n",
    "\n",
    "    return spect\n",
    "\n",
    "def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n",
    "  \n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
    "\n",
    "    samples = 0\n",
    "    for root, _, files in _recursive_list(directory):\n",
    "        for fname in files:\n",
    "            is_valid = False\n",
    "            for extension in white_list_formats:\n",
    "                if fname.lower().endswith('.' + extension):\n",
    "                    is_valid = True\n",
    "                    break\n",
    "            if is_valid:\n",
    "                samples += 1\n",
    "    return samples\n",
    "\n",
    "def _list_valid_filenames_in_directory(directory, white_list_formats,\n",
    "                                       class_indices, follow_links):\n",
    " \n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
    "\n",
    "    classes = []\n",
    "    filenames = []\n",
    "    subdir = os.path.basename(directory)\n",
    "    basedir = os.path.dirname(directory)\n",
    "    for root, _, files in _recursive_list(directory):\n",
    "        for fname in sorted(files):\n",
    "            is_valid = False\n",
    "            for extension in white_list_formats:\n",
    "                if fname.lower().endswith('.' + extension):\n",
    "                    is_valid = True\n",
    "                    break\n",
    "            if is_valid:\n",
    "                classes.append(class_indices[subdir])\n",
    "                # add filename relative to directory\n",
    "                absolute_path = os.path.join(root, fname)\n",
    "                filenames.append(os.path.relpath(absolute_path, basedir))\n",
    "    return classes, filenames\n",
    "\n",
    "class SpeechDirectoryIterator(Iterator):\n",
    "    \n",
    "\n",
    "    def __init__(self, directory, window_size, window_stride, \n",
    "                 window_type, normalize, max_len=101,\n",
    "                 target_size=(256, 256), color_mode='grayscale',\n",
    "                 classes=None, class_mode='categorical',\n",
    "                 batch_size=32, shuffle=True, seed=None,\n",
    "                 data_format=None, save_to_dir=None,\n",
    "                 save_prefix='', save_format='png',\n",
    "                 follow_links=False, interpolation='nearest', augment=False,\n",
    "                allow_speedandpitch = False, allow_pitch = False,\n",
    "                allow_speed = False, allow_dyn = False, allow_noise = False, allow_timeshift=False ):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_type = window_type\n",
    "        self.normalize = normalize\n",
    "        self.max_len = max_len\n",
    "        self.directory = directory\n",
    "        self.allow_speedandpitch = allow_speedandpitch\n",
    "        self.allow_pitch = allow_pitch\n",
    "        self.allow_speed = allow_speed \n",
    "        self.allow_dyn = allow_dyn\n",
    "        self.allow_noise = allow_noise\n",
    "        self.allow_timeshift = allow_timeshift \n",
    "        self.augment = augment\n",
    "#        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "        if color_mode not in {'rgb', 'grayscale'}:\n",
    "            raise ValueError('Invalid color mode:', color_mode,\n",
    "                             '; expected \"rgb\" or \"grayscale\".')\n",
    "        self.color_mode = color_mode\n",
    "        self.data_format = data_format\n",
    "        if self.color_mode == 'rgb':\n",
    "            if self.data_format == 'channels_last':\n",
    "                self.image_shape = self.target_size + (3,)\n",
    "            else:\n",
    "                self.image_shape = (3,) + self.target_size\n",
    "        else:\n",
    "            if self.data_format == 'channels_last':\n",
    "                self.image_shape = self.target_size + (1,)\n",
    "            else:\n",
    "                self.image_shape = (1,) + self.target_size\n",
    "        self.classes = classes\n",
    "        if class_mode not in {'categorical', 'binary', 'sparse',\n",
    "                              'input', None}:\n",
    "            raise ValueError('Invalid class_mode:', class_mode,\n",
    "                             '; expected one of \"categorical\", '\n",
    "                             '\"binary\", \"sparse\", \"input\"'\n",
    "                             ' or None.')\n",
    "        self.class_mode = class_mode\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'wav'}\n",
    "\n",
    "        # first, count the number of samples and classes\n",
    "        self.samples = 0\n",
    "\n",
    "        if not classes:\n",
    "            classes = []\n",
    "            for subdir in sorted(os.listdir(directory)):\n",
    "                if os.path.isdir(os.path.join(directory, subdir)):\n",
    "                    classes.append(subdir)\n",
    "        self.num_classes = len(classes)\n",
    "        self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "\n",
    "        pool = multiprocessing.pool.ThreadPool()\n",
    "        function_partial = partial(_count_valid_files_in_directory,\n",
    "                                   white_list_formats=white_list_formats,\n",
    "                                   follow_links=follow_links)\n",
    "        self.samples = sum(pool.map(function_partial,\n",
    "                                    (os.path.join(directory, subdir)\n",
    "                                     for subdir in classes)))\n",
    "\n",
    "        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))\n",
    "\n",
    "        # second, build an index of the images in the different class subfolders\n",
    "        results = []\n",
    "\n",
    "        self.filenames = []\n",
    "        self.classes = np.zeros((self.samples,), dtype='int32')\n",
    "        i = 0\n",
    "        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n",
    "            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n",
    "                                            (dirpath, white_list_formats,\n",
    "                                             self.class_indices, follow_links)))\n",
    "            \n",
    "        \n",
    "        for res in results:\n",
    "            classes, filenames = res.get()\n",
    "            self.classes[i:i + len(classes)] = classes\n",
    "            self.filenames += filenames\n",
    "            if i==0:\n",
    "                img = spect_loader(os.path.join(self.directory, filenames[0]), \n",
    "                               self.window_size, \n",
    "                               self.window_stride, \n",
    "                               self.window_type, \n",
    "                               self.normalize, \n",
    "                               self.max_len, \n",
    "                               self.augment,\n",
    "                               self.allow_speedandpitch,\n",
    "                               self.allow_pitch,\n",
    "                               self.allow_speed, \n",
    "                               self.allow_dyn,\n",
    "                               self.allow_noise,\n",
    "                               self.allow_timeshift ) \n",
    "                img=np.swapaxes(img, 0, 2)\n",
    "                self.target_size = tuple((img.shape[0], img.shape[1]))\n",
    "                print(self.target_size)\n",
    "                if self.color_mode == 'rgb':\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        self.image_shape = self.target_size + (3,)\n",
    "                    else:\n",
    "                        self.image_shape = (3,) + self.target_size\n",
    "                else:\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        self.image_shape = self.target_size + (1,)\n",
    "                    else:\n",
    "                        self.image_shape = (1,) + self.target_size\n",
    "                        \n",
    "            i += len(classes)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        super(SpeechDirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        batch_f = []\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "        # build batch of image data\n",
    "        #print(index_array)\n",
    "        for i, j in enumerate(index_array):\n",
    "            #print(i, j, self.filenames[j])\n",
    "            fname = self.filenames[j]\n",
    "            #img = load_img(os.path.join(self.directory, fname),\n",
    "            #               grayscale=grayscale,\n",
    "            #               target_size=self.target_size,\n",
    "            #               interpolation=self.interpolation)\n",
    "            img = spect_loader(os.path.join(self.directory, fname), \n",
    "                               self.window_size, \n",
    "                               self.window_stride, \n",
    "                               self.window_type, \n",
    "                               self.normalize, \n",
    "                               self.max_len, \n",
    "                                )\n",
    "            img=np.swapaxes(img, 0, 2)\n",
    "            \n",
    "            x = img_to_array(img, data_format=self.data_format)\n",
    "           \n",
    "            batch_x[i] = x\n",
    "            batch_f.append(fname)\n",
    "       \n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                img = array_to_img(batch_x[i], self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n",
    "                                                                  index=j,\n",
    "                                                                  hash=np.random.randint(1e7),\n",
    "                                                                  format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "       \n",
    "        if self.class_mode == 'input':\n",
    "            batch_y = batch_x.copy()\n",
    "        elif self.class_mode == 'sparse':\n",
    "            batch_y = self.classes[index_array]\n",
    "        elif self.class_mode == 'binary':\n",
    "            batch_y = self.classes[index_array].astype(K.floatx())\n",
    "        elif self.class_mode == 'categorical':\n",
    "            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n",
    "            for i, label in enumerate(self.classes[index_array]):\n",
    "                batch_y[i, label] = 1.\n",
    "        else:\n",
    "            return batch_x\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)[0]\n",
    "      \n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21585 images belonging to 12 classes.\n",
      "(101, 161)\n"
     ]
    }
   ],
   "source": [
    "window_size=.02\n",
    "window_stride=.01\n",
    "window_type='hamming'\n",
    "normalize=True\n",
    "max_len=101\n",
    "batch_size = 64\n",
    "train_iterator = SpeechDirectoryIterator(directory=train_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5361 images belonging to 12 classes.\n",
      "(101, 161)\n"
     ]
    }
   ],
   "source": [
    "val_iterator = SpeechDirectoryIterator(directory=val_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101, 161, 1)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 101, 161, 1)       4         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 97, 157, 32)       832       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 93, 153, 32)       25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 31, 51, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 31, 51, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 29, 49, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 27, 47, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 15, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 15, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 11, 64)         102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 228,144\n",
      "Trainable params: 227,630\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "shape = (99, 81, 1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "input_shape = (99, 81, 1)\n",
    "nclass = 12\n",
    "inp = Input(shape=train_iterator.image_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(16, kernel_size=5, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(16, kernel_size=5, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)\n",
    "img_1 = Dropout(rate=0.4)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=5, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(32, kernel_size=5, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)\n",
    "img_1 = Dropout(rate=0.4)(img_1)\n",
    "img_1 = Convolution2D(64, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(64, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)\n",
    "img_1 = Dropout(rate=0.4)(img_1)\n",
    "img_1 = Convolution2D(64, kernel_size=5, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)\n",
    "img_1 = Dropout(rate=0.4)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(len(classnames), activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "338/338 [==============================] - 636s 2s/step - loss: 2.1857 - acc: 0.2595 - val_loss: 2.3067 - val_acc: 0.1666\n",
      "Epoch 2/60\n",
      "338/338 [==============================] - 886s 3s/step - loss: 1.3971 - acc: 0.5010 - val_loss: 1.0444 - val_acc: 0.6430\n",
      "Epoch 3/60\n",
      "338/338 [==============================] - 936s 3s/step - loss: 1.0054 - acc: 0.6453 - val_loss: 0.7594 - val_acc: 0.7558\n",
      "Epoch 4/60\n",
      "338/338 [==============================] - 3720s 11s/step - loss: 0.8201 - acc: 0.7142 - val_loss: 0.6215 - val_acc: 0.8002\n",
      "Epoch 5/60\n",
      "338/338 [==============================] - 525s 2s/step - loss: 0.7096 - acc: 0.7577 - val_loss: 0.5303 - val_acc: 0.8329\n",
      "Epoch 6/60\n",
      "338/338 [==============================] - 688s 2s/step - loss: 0.6463 - acc: 0.7787 - val_loss: 0.5063 - val_acc: 0.8435\n",
      "Epoch 7/60\n",
      "338/338 [==============================] - 812s 2s/step - loss: 0.5949 - acc: 0.8001 - val_loss: 0.4762 - val_acc: 0.8465\n",
      "Epoch 8/60\n",
      "338/338 [==============================] - 873s 3s/step - loss: 0.5412 - acc: 0.8171 - val_loss: 0.4216 - val_acc: 0.8666\n",
      "Epoch 9/60\n",
      "338/338 [==============================] - 891s 3s/step - loss: 0.5183 - acc: 0.8254 - val_loss: 0.4058 - val_acc: 0.8730\n",
      "Epoch 10/60\n",
      "338/338 [==============================] - 895s 3s/step - loss: 0.5063 - acc: 0.8293 - val_loss: 0.4138 - val_acc: 0.8786\n",
      "Epoch 11/60\n",
      "338/338 [==============================] - 945s 3s/step - loss: 0.4774 - acc: 0.8402 - val_loss: 0.3731 - val_acc: 0.8776\n",
      "Epoch 12/60\n",
      "338/338 [==============================] - 893s 3s/step - loss: 0.4525 - acc: 0.8495 - val_loss: 0.3727 - val_acc: 0.8797\n",
      "Epoch 13/60\n",
      "338/338 [==============================] - 917s 3s/step - loss: 0.4285 - acc: 0.8549 - val_loss: 0.3638 - val_acc: 0.8806\n",
      "Epoch 14/60\n",
      "338/338 [==============================] - 943s 3s/step - loss: 0.4247 - acc: 0.8595 - val_loss: 0.4134 - val_acc: 0.8713\n",
      "Epoch 15/60\n",
      "338/338 [==============================] - 1001s 3s/step - loss: 0.4056 - acc: 0.8627 - val_loss: 0.3478 - val_acc: 0.8901\n",
      "Epoch 16/60\n",
      "338/338 [==============================] - 995s 3s/step - loss: 0.3903 - acc: 0.8692 - val_loss: 0.3447 - val_acc: 0.8944\n",
      "Epoch 17/60\n",
      "338/338 [==============================] - 971s 3s/step - loss: 0.3850 - acc: 0.8700 - val_loss: 0.3527 - val_acc: 0.8929\n",
      "Epoch 18/60\n",
      "338/338 [==============================] - 986s 3s/step - loss: 0.3775 - acc: 0.8736 - val_loss: 0.3272 - val_acc: 0.8954\n",
      "Epoch 19/60\n",
      "338/338 [==============================] - 995s 3s/step - loss: 0.3568 - acc: 0.8825 - val_loss: 0.3240 - val_acc: 0.8970\n",
      "Epoch 20/60\n",
      "338/338 [==============================] - 1011s 3s/step - loss: 0.3541 - acc: 0.8843 - val_loss: 0.3256 - val_acc: 0.8978\n",
      "Epoch 21/60\n",
      "338/338 [==============================] - 934s 3s/step - loss: 0.3609 - acc: 0.8808 - val_loss: 0.3440 - val_acc: 0.8974\n",
      "Epoch 22/60\n",
      "338/338 [==============================] - 931s 3s/step - loss: 0.3442 - acc: 0.8860 - val_loss: 0.3102 - val_acc: 0.9030\n",
      "Epoch 23/60\n",
      "338/338 [==============================] - 948s 3s/step - loss: 0.3346 - acc: 0.8895 - val_loss: 0.3194 - val_acc: 0.8983\n",
      "Epoch 24/60\n",
      "338/338 [==============================] - 969s 3s/step - loss: 0.3368 - acc: 0.8857 - val_loss: 0.3301 - val_acc: 0.8974\n",
      "Epoch 25/60\n",
      "338/338 [==============================] - 1027s 3s/step - loss: 0.3334 - acc: 0.8909 - val_loss: 0.3206 - val_acc: 0.8982\n",
      "Epoch 26/60\n",
      "337/338 [============================>.] - ETA: 2s - loss: 0.3217 - acc: 0.8919\n",
      "Epoch 00026: reducing learning rate to 0.0005000000237487257.\n",
      "338/338 [==============================] - 972s 3s/step - loss: 0.3217 - acc: 0.8919 - val_loss: 0.3570 - val_acc: 0.8957\n",
      "Epoch 27/60\n",
      "338/338 [==============================] - 1018s 3s/step - loss: 0.2872 - acc: 0.9036 - val_loss: 0.2946 - val_acc: 0.9080\n",
      "Epoch 28/60\n",
      "338/338 [==============================] - 1028s 3s/step - loss: 0.2771 - acc: 0.9081 - val_loss: 0.2996 - val_acc: 0.9067\n",
      "Epoch 29/60\n",
      "338/338 [==============================] - 1005s 3s/step - loss: 0.2692 - acc: 0.9090 - val_loss: 0.2939 - val_acc: 0.9084\n",
      "Epoch 30/60\n",
      "338/338 [==============================] - 981s 3s/step - loss: 0.2666 - acc: 0.9096 - val_loss: 0.2866 - val_acc: 0.9097\n",
      "Epoch 31/60\n",
      "338/338 [==============================] - 992s 3s/step - loss: 0.2642 - acc: 0.9092 - val_loss: 0.2796 - val_acc: 0.9172\n",
      "Epoch 32/60\n",
      "338/338 [==============================] - 1040s 3s/step - loss: 0.2607 - acc: 0.9131 - val_loss: 0.2797 - val_acc: 0.9103\n",
      "Epoch 33/60\n",
      "338/338 [==============================] - 985s 3s/step - loss: 0.2556 - acc: 0.9135 - val_loss: 0.2819 - val_acc: 0.9129\n",
      "Epoch 34/60\n",
      "338/338 [==============================] - 1010s 3s/step - loss: 0.2484 - acc: 0.9166 - val_loss: 0.2783 - val_acc: 0.9118\n",
      "Epoch 35/60\n",
      "338/338 [==============================] - 1018s 3s/step - loss: 0.2464 - acc: 0.9175 - val_loss: 0.2928 - val_acc: 0.9095\n",
      "Epoch 36/60\n",
      "338/338 [==============================] - 985s 3s/step - loss: 0.2559 - acc: 0.9139 - val_loss: 0.2886 - val_acc: 0.9127\n",
      "Epoch 37/60\n",
      "338/338 [==============================] - 1060s 3s/step - loss: 0.2484 - acc: 0.9147 - val_loss: 0.2743 - val_acc: 0.9157\n",
      "Epoch 38/60\n",
      "338/338 [==============================] - 982s 3s/step - loss: 0.2386 - acc: 0.9196 - val_loss: 0.2696 - val_acc: 0.9153\n",
      "Epoch 39/60\n",
      "338/338 [==============================] - 1066s 3s/step - loss: 0.2364 - acc: 0.9212 - val_loss: 0.2843 - val_acc: 0.9151\n",
      "Epoch 40/60\n",
      "338/338 [==============================] - 990s 3s/step - loss: 0.2396 - acc: 0.9188 - val_loss: 0.2734 - val_acc: 0.9134\n",
      "Epoch 41/60\n",
      "338/338 [==============================] - 964s 3s/step - loss: 0.2433 - acc: 0.9166 - val_loss: 0.2679 - val_acc: 0.9159\n",
      "Epoch 42/60\n",
      "338/338 [==============================] - 1048s 3s/step - loss: 0.2337 - acc: 0.9230 - val_loss: 0.2631 - val_acc: 0.9153\n",
      "Epoch 43/60\n",
      "338/338 [==============================] - 1003s 3s/step - loss: 0.2395 - acc: 0.9196 - val_loss: 0.2787 - val_acc: 0.9142\n",
      "Epoch 44/60\n",
      "338/338 [==============================] - 1025s 3s/step - loss: 0.2301 - acc: 0.9226 - val_loss: 0.2687 - val_acc: 0.9166\n",
      "Epoch 45/60\n",
      "338/338 [==============================] - 1017s 3s/step - loss: 0.2324 - acc: 0.9210 - val_loss: 0.2779 - val_acc: 0.9151\n",
      "Epoch 46/60\n",
      "337/338 [============================>.] - ETA: 2s - loss: 0.2248 - acc: 0.9238\n",
      "Epoch 00046: reducing learning rate to 0.0002500000118743628.\n",
      "338/338 [==============================] - 1013s 3s/step - loss: 0.2244 - acc: 0.9240 - val_loss: 0.2689 - val_acc: 0.9190\n",
      "Epoch 47/60\n",
      "338/338 [==============================] - 1017s 3s/step - loss: 0.2084 - acc: 0.9297 - val_loss: 0.2611 - val_acc: 0.9211\n",
      "Epoch 48/60\n",
      "338/338 [==============================] - 1045s 3s/step - loss: 0.2069 - acc: 0.9306 - val_loss: 0.2681 - val_acc: 0.9179\n",
      "Epoch 49/60\n",
      "338/338 [==============================] - 1072s 3s/step - loss: 0.2101 - acc: 0.9295 - val_loss: 0.2717 - val_acc: 0.9181\n",
      "Epoch 50/60\n",
      "338/338 [==============================] - 1020s 3s/step - loss: 0.2050 - acc: 0.9298 - val_loss: 0.2694 - val_acc: 0.9190\n",
      "Epoch 51/60\n",
      "337/338 [============================>.] - ETA: 2s - loss: 0.2022 - acc: 0.9301\n",
      "Epoch 00051: reducing learning rate to 0.0001250000059371814.\n",
      "338/338 [==============================] - 1012s 3s/step - loss: 0.2020 - acc: 0.9301 - val_loss: 0.2692 - val_acc: 0.9192\n",
      "Epoch 52/60\n",
      "338/338 [==============================] - 1031s 3s/step - loss: 0.1989 - acc: 0.9327 - val_loss: 0.2572 - val_acc: 0.9217\n",
      "Epoch 53/60\n",
      "338/338 [==============================] - 1015s 3s/step - loss: 0.1897 - acc: 0.9359 - val_loss: 0.2617 - val_acc: 0.9215\n",
      "Epoch 54/60\n",
      "338/338 [==============================] - 977s 3s/step - loss: 0.1950 - acc: 0.9333 - val_loss: 0.2618 - val_acc: 0.9204\n",
      "Epoch 55/60\n",
      "338/338 [==============================] - 994s 3s/step - loss: 0.1917 - acc: 0.9368 - val_loss: 0.2655 - val_acc: 0.9202\n",
      "Epoch 56/60\n",
      "337/338 [============================>.] - ETA: 2s - loss: 0.1908 - acc: 0.9332\n",
      "Epoch 00056: reducing learning rate to 6.25000029685907e-05.\n",
      "338/338 [==============================] - 981s 3s/step - loss: 0.1908 - acc: 0.9332 - val_loss: 0.2645 - val_acc: 0.9215\n",
      "Epoch 57/60\n",
      "338/338 [==============================] - 946s 3s/step - loss: 0.1884 - acc: 0.9377 - val_loss: 0.2648 - val_acc: 0.9202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba839e7a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../logs', histogram_freq=0,write_graph=True, write_images=False)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='auto', min_lr=0.00001)\n",
    "\n",
    "model.fit_generator(train_iterator,\n",
    "        steps_per_epoch=int(np.ceil(train_iterator.n / batch_size)),\n",
    "        epochs=60,\n",
    "        validation_data=val_iterator,\n",
    "        validation_steps=int(np.ceil(val_iterator.n / batch_size)),\n",
    "        verbose=1, callbacks=[early, reduce,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "test_path_audio=os.path.join(test_path, 'audio')\n",
    "test_filenames = os.listdir(test_path_audio) \n",
    "test_filenames=np.sort(test_filenames)\n",
    "list(test_filenames)[:10]\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../speech/speech_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"speech_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2478/2478 [==============================] - 2143s 865ms/step\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "def loadAndSpect(fname,  window_size, window_stride, window_type, normalize, max_len):\n",
    "    img = spect_loader(os.path.join(test_path_audio, fname), \n",
    "                       window_size, \n",
    "                       window_stride, \n",
    "                       window_type, \n",
    "                       normalize, \n",
    "                       max_len)\n",
    "    img=np.swapaxes(img, 0, 2)\n",
    "\n",
    "    x = img_to_array(img, data_format='channels_last')\n",
    "    return x\n",
    "            \n",
    "class WavSequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, batch_size=64, window_size=0.02, window_stride=0.01, window_type='hamming', \n",
    "                 normalize=True, max_len=101):\n",
    "        self.x = x_set\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_type = window_type\n",
    "        self.normalize = normalize\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            loadAndSpect(file_name, window_size, window_stride, window_type, normalize, max_len)\n",
    "               for file_name in batch_x])\n",
    "\n",
    "seq = WavSequence(test_filenames, batch_size=batch_size)\n",
    "preds = model.predict_generator(generator=seq, \n",
    "                        steps=len(seq), \n",
    "                        workers=1, \n",
    "                        use_multiprocessing=False, \n",
    "                        verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'down', 1: 'go', 2: 'left', 3: 'no', 4: 'off', 5: 'on', 6: 'right', 7: 'silence', 8: 'stop', 9: 'unknown', 10: 'up', 11: 'yes'}\n",
      "[3 9 9 4 9 9 5 9 1 6]\n",
      "[ 0.99950683  0.98696923  0.97481954  0.28996414  0.99342221  0.94035625\n",
      "  0.86540395  0.57972056  0.99735045  0.15348034]\n",
      "[[    0     1     2     3     4     5     6     7     8     9    10    11]\n",
      " [ 5392  8297  8593  8564  7846  8250 10520  8511  6793 66951 13041  5780]]\n"
     ]
    }
   ],
   "source": [
    "inv_map = {v: k for k, v in train_iterator.class_indices.items()}\n",
    "print(inv_map)\n",
    "classes = np.argmax(preds, axis=1)\n",
    "probes = np.max(preds, axis=1)\n",
    "print (classes[:10])\n",
    "print (probes[:10])\n",
    "\n",
    "unique_elements, counts_elements = np.unique(classes, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "res = []\n",
    "for cl in classes:\n",
    "    res.append(inv_map[cl])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.transpose(np.vstack((np.array(test_filenames), res))), columns=['fname', 'label'])\n",
    "df.to_csv('../output/speech_result.csv', header=True, quoting=0, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
