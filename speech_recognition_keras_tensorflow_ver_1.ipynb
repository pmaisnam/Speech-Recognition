{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Speech Recognition \n",
    "\n",
    ".Speech recognition Keras model. Given a short speech map it to short text.\n",
    ".Download data from https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Input, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    \n",
    "    datadir = Path(datadir)\n",
    "    files = [(str(f), f.parts[-2]) for f in datadir.glob('*/*.wav') if f]\n",
    "    dframe = pd.DataFrame(files, columns=['path', 'word'])\n",
    "    \n",
    "    speech_commands = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "    \n",
    "    words = dframe.word.unique().tolist()\n",
    "    silence = ['_background_noise_']\n",
    "    unknown = [w for w in words if w not in silence + speech_commands]\n",
    "\n",
    "   \n",
    "    dframe.loc[dframe.word.isin(silence), 'word'] = 'unknown'\n",
    "    dframe.loc[dframe.word.isin(unknown), 'word'] = 'unknown'\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "def get_specgrams(paths, nsamples=16000):\n",
    "    \n",
    "    wavs = [wavfile.read(x)[1] for x in paths]\n",
    "\n",
    "    # zero pad the shorter samples and cut off the long ones.\n",
    "    data = [] \n",
    "    for wav in wavs:\n",
    "        if wav.size < 16000:\n",
    "            d = np.pad(wav, (nsamples - wav.size, 0), mode='constant')\n",
    "        else:\n",
    "            d = wav[0:nsamples]\n",
    "        data.append(d)\n",
    "\n",
    "    # get the specgram\n",
    "    specgram = [signal.spectrogram(d, nperseg=256, noverlap=128)[2] for d in data]\n",
    "    specgram = [s.reshape(129, 124, -1) for s in specgram]\n",
    "    \n",
    "    return specgram\n",
    "\n",
    "\n",
    "def batch_generator(X, y, batch_size=16):\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        # choose batch_size random images / labels from the data\n",
    "        idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "        im = X[idx]\n",
    "        label = y[idx]\n",
    "        \n",
    "        specgram = get_specgrams(im)\n",
    "\n",
    "\n",
    "        yield np.concatenate([specgram]), label\n",
    "\n",
    "\n",
    "def build_model(shape):\n",
    "    '''Create a keras model.'''\n",
    "    inputlayer = Input(shape=shape)\n",
    "\n",
    "    model = BatchNormalization()(inputlayer)\n",
    "    #Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "    model = Conv2D(16, (3, 3), activation='elu')(model)\n",
    "    model = Dropout(0.25)(model)\n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(32, activation='elu')(model)\n",
    "    model = Dropout(0.8)(model)\n",
    "    \n",
    "    # 11 because background noise has been taken out\n",
    "    model = Dense(11, activation='sigmoid')(model)\n",
    "    \n",
    "    model = Model(inputs=inputlayer, outputs=model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "train = load_data('../train/audio/')\n",
    "shape = (129, 124, 1)\n",
    "model = build_model(shape)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelbinarizer = LabelBinarizer()\n",
    "X = train.path\n",
    "y = labelbinarizer.fit_transform(train.word)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 776/1415 [===============>..............] - ETA: 445s - loss: 0.2908 - acc: 0.9037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415/1415 [==============================] - 1169s - loss: 0.2636 - acc: 0.9165 - val_loss: 0.2346 - val_acc: 0.9325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc9cd78da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = batch_generator(Xtrain.values, ytrain, batch_size=32)\n",
    "valid_gen = batch_generator(Xtest.values, ytest, batch_size=32)\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=Xtrain.shape[0] // 32,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=Xtest.shape[0] // 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test = load_data('../test/')\n",
    "\n",
    "predictions = []\n",
    "data_dir = test.path.tolist()\n",
    "\n",
    "for filePath in data_dir:\n",
    "    specgram = get_specgrams([filePath])\n",
    "    pred = model.predict(np.array(specgram))\n",
    "    predictions.extend(pred)\n",
    "\n",
    "\n",
    "labels = [labelbinarizer.inverse_transform(p.reshape(1, -1), threshold=0.5)[0] for p in predictions]\n",
    "test['labels'] = labels\n",
    "\n",
    "test.path = test.path.apply(lambda x: str(x).split('/')[-1])\n",
    "submission = pd.DataFrame({'fname': test.path.tolist(), 'label': labels})\n",
    "submission.to_csv('../output/speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
